{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc62ff28",
   "metadata": {},
   "source": [
    "# ___________Proyecto completo___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c230fbb",
   "metadata": {},
   "source": [
    "# Pipeline de reconocimiento facial\n",
    "\n",
    "\n",
    "En los apartado teórico, se ha descrito de forma separada cada uno de los pasos que forman parte del proceso de reconocimiento facial. Sin embargo, para poder aplicarlo en un sistema real, es necesario combinarlos todos para que se ejecuten de forma secuencial. Con este objetivo, se definen una serie de funciones que permitan automatizar el proceso completo.\n",
    "\n",
    "* pipeline_deteccion_imagen(): detectar e identificar las personas de una imagen.\n",
    "\n",
    "* pipeline_deteccion_video(): detectar e identificar las personas de un vídeo.\n",
    "\n",
    "* pipeline_deteccion_webcam(): detectar e identificar las personas de un vídeo de entrada por webcam. Esta función requiere de abrir una ventana emergente de visualización, por lo que no puede utilizarse en google colab.\n",
    "\n",
    "* crear_diccionario_referencias(): crear un diccionario con los embeddings de referencia para cada persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81dde255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "# ==============================================================================\n",
    "import numpy as np # Manipulación eficiente de arrays multidimensionales.\n",
    "import cv2 # OpenCV, para procesamiento de imágenes y visión por computadora.\n",
    "import matplotlib.pyplot as plt # Visualización de datos y gráficos.\n",
    "import torch # PyTorch, biblioteca de aprendizaje automático y computación numérica.\n",
    "import warnings # Control de advertencias durante la ejecución del código.\n",
    "import typing # Anotaciones de tipos para Python (Python 3.5+).\n",
    "import logging # Registro de eventos y mensajes durante la ejecución del código.\n",
    "import os # Funciones para interactuar con el sistema operativo.\n",
    "import platform # Información sobre la plataforma y el sistema operativo en uso.\n",
    "import glob # Búsqueda de archivos mediante patrones de nombres.\n",
    "import PIL # Python Imaging Library, para manipulación de imágenes.\n",
    "import facenet_pytorch # Implementación de FaceNet en PyTorch para reconocimiento facial.\n",
    "from typing import Union, Dict # Anotaciones de tipos para Python (Python 3.5+).\n",
    "from PIL import Image # Importar la clase Image de la biblioteca PIL.\n",
    "from facenet_pytorch import MTCNN # Importar el detector de caras MTCNN de facenet_pytorch.\n",
    "from facenet_pytorch import InceptionResnetV1 # Importar el modelo InceptionResnetV1 de facenet_pytorch.\n",
    "from urllib.request import urlretrieve # Descarga de archivos desde URLs.\n",
    "from tqdm import tqdm  # Barra de progreso para iteraciones.\n",
    "from scipy.spatial.distance import euclidean # Cálculo de la distancia euclidiana entre vectores.\n",
    "from scipy.spatial.distance import cosine # Cálculo de la distancia del coseno entre vectores.\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "####################################################################################\n",
    "# Configuración básica del registro (logging) para monitorear la ejecución del codigo\n",
    "# ==============================================================================\n",
    "\n",
    "# Establecer el formato del registro con fecha, nombre del logger, nivel de registro y mensaje\n",
    "# '%(asctime)-5s': Fecha y hora en el formato especificado (-5s para limitar el ancho del campo)\n",
    "# '%(name)-10s': Nombre del logger (máximo 10 caracteres)\n",
    "# '%(levelname)-5s': Nivel de registro del mensaje (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "# '%(message)s': El mensaje de registro\n",
    "format = '%(asctime)-5s %(name)-10s %(levelname)-5s %(message)s'\n",
    "\n",
    "# Establecer el nivel de registro global del logger\n",
    "# logging.WARNING: Configurar el nivel de registro a WARNING (o superior)\n",
    "# Esto significa que solo se registrarán mensajes con nivel WARNING, ERROR y CRITICAL\n",
    "level = logging.WARNING\n",
    "# Configurar la configuración básica del registro con el formato y nivel especificados\n",
    "logging.basicConfig(format=format, level=level)\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# Funciones para la detección, extracción, embedding, identificación y gráficos\n",
    "# ==============================================================================\n",
    "def detectar_caras(imagen: Union[PIL.Image.Image, np.ndarray],\n",
    "                   detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
    "                   keep_all: bool        = True,\n",
    "                   min_face_size: int    = 20,\n",
    "                   thresholds: list      = [0.6, 0.7, 0.7],\n",
    "                   device: str           = None,\n",
    "                   min_confidence: float = 0.5,\n",
    "                   fix_bbox: bool        = True,\n",
    "                   verbose               = False)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detectar la posición de caras en una imagen empleando un detector MTCNN.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    imagen: PIL.Image, np.ndarray\n",
    "        PIL Image o numpy array con la representación de la imagen.\n",
    "\n",
    "    detector : facenet_pytorch.models.mtcnn.MTCNN\n",
    "        Default: None\n",
    "        Modelo ``MTCNN`` empleado para detectar las caras de la imagen. Si es\n",
    "        ``None`` se inicializa uno nuevo. La inicialización del modelo puede\n",
    "        tardar varios segundos, por lo que, en ciertos escenarios, es preferible\n",
    "        inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    keep_all: bool\n",
    "        Default: True\n",
    "        Si `True`, se devuelven todas las caras detectadas en la imagen.\n",
    "\n",
    "    min_face_size : int\n",
    "        Default: 20\n",
    "        Tamaño mínimo de que deben tener las caras para ser detectadas por la red \n",
    "        MTCNN.\n",
    "        \n",
    "    thresholds: list\n",
    "        Default: [0.6, 0.7, 0.7]\n",
    "        Límites de detección de cada una de las 3 redes que forman el detector MTCNN.\n",
    "    \n",
    "    device: str\n",
    "        Default: None\n",
    "        Device donde se ejecuta el modelo. Si el detector MTCNN, se pasa como\n",
    "        argumento, no es necesario.\n",
    "\n",
    "    min_confidence : float\n",
    "        Default: 0.5\n",
    "        confianza (probabilidad) mínima que debe de tener la cara detectada para\n",
    "        que se incluya en los resultados.\n",
    "\n",
    "    fix_bbox : bool\n",
    "        Default: True\n",
    "        Acota las dimensiones de las bounding box para que no excedan las\n",
    "        dimensiones de la imagen. Esto evita problemas cuando se intenta\n",
    "        representar las bounding box de caras que están en el margen de la\n",
    "        imagen.\n",
    "\n",
    "    verbose : bool\n",
    "        Default: False\n",
    "        Mostrar información del proceso por pantalla.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    numpy.ndarray\n",
    "        Numpy array con las bounding box de cada cara detectada. Cada bounding\n",
    "        box es a su vez una array formada por 4 valores que definen las coordenadas\n",
    "        de la esquina superior-izquierda y la esquina inferior-derecha.\n",
    "        \n",
    "             (box[0],box[1])------------\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    ------------(box[0],box[1])\n",
    "                    \n",
    "        Las bounding box devueltas por el detector ``MTCNN`` están definidas por\n",
    "        valores de tipo `float`. Esto supone un problema para la posterior \n",
    "        representación con matplotlib, por lo que se convierten a tipo `int`.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
    "        raise Exception(\n",
    "            f\"`imagen` debe ser `np.ndarray, PIL.Image`. Recibido {type(imagen)}.\"\n",
    "        )\n",
    "\n",
    "    if detector is None:\n",
    "        logging.info('Iniciando detector MTCC')\n",
    "        detector = MTCNN(\n",
    "                        keep_all      = keep_all,\n",
    "                        min_face_size = min_face_size,\n",
    "                        thresholds    = thresholds,\n",
    "                        post_process  = False,\n",
    "                        device        = device\n",
    "                   )\n",
    "        \n",
    "    # Detección de caras\n",
    "    # --------------------------------------------------------------------------\n",
    "    if isinstance(imagen, PIL.Image.Image):\n",
    "        imagen = np.array(imagen).astype(np.float32)\n",
    "        \n",
    "    bboxes, probs = detector.detect(imagen, landmarks=False)\n",
    "    \n",
    "    if bboxes is None:\n",
    "        bboxes = np.array([])\n",
    "        probs  = np.array([])\n",
    "    else:\n",
    "        # Se descartan caras con una probabilidad estimada inferior a `min_confidence`.\n",
    "        bboxes = bboxes[probs > min_confidence]\n",
    "        probs  = probs[probs > min_confidence]\n",
    "        \n",
    "    logging.info(f'Número total de caras detectadas: {len(bboxes)}')\n",
    "    logging.info(f'Número final de caras seleccionadas: {len(bboxes)}')\n",
    "\n",
    "    # Corregir bounding boxes\n",
    "    #---------------------------------------------------------------------------\n",
    "    # Si alguna de las esquinas de la bounding box está fuera de la imagen, se\n",
    "    # corrigen para que no sobrepase los márgenes.\n",
    "    if len(bboxes) > 0 and fix_bbox:       \n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            if bbox[0] < 0:\n",
    "                bboxes[i][0] = 0\n",
    "            if bbox[1] < 0:\n",
    "                bboxes[i][1] = 0\n",
    "            if bbox[2] > imagen.shape[1]:\n",
    "                bboxes[i][2] = imagen.shape[1]\n",
    "            if bbox[3] > imagen.shape[0]:\n",
    "                bboxes[i][3] = imagen.shape[0]\n",
    "\n",
    "    # Información de proceso\n",
    "    # ----------------------------------------------------------------------\n",
    "    if verbose:\n",
    "        print(\"----------------\")\n",
    "        print(\"Imagen escaneada\")\n",
    "        print(\"----------------\")\n",
    "        print(f\"Caras detectadas: {len(bboxes)}\")\n",
    "        print(f\"Correción bounding boxes: {ix_bbox}\")\n",
    "        print(f\"Coordenadas bounding boxes: {bboxes}\")\n",
    "        print(f\"Confianza bounding boxes:{probs} \")\n",
    "        print(\"\")\n",
    "        \n",
    "    return bboxes.astype(int)\n",
    "\n",
    "\n",
    "def mostrar_bboxes(imagen: Union[PIL.Image.Image, np.ndarray],\n",
    "                   bboxes: np.ndarray,\n",
    "                   identidades: list=None,\n",
    "                   ax=None ) -> None:\n",
    "    \"\"\"\n",
    "    Mostrar la imagen original con las boundig box de las caras detectadas\n",
    "    empleando matplotlib. Si pasa las identidades, se muestran sobre cada\n",
    "    bounding box.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    imagen: PIL.Image, np.ndarray\n",
    "        `PIL Image` o `numpy array` con la representación de la imagen.\n",
    "    \n",
    "    bboxes: np.array\n",
    "        Numpy array con las bounding box de las caras presentes en las imágenes.\n",
    "        Cada bounding box es a su vez una array formada por 4 valores que definen\n",
    "        las coordenadas de la esquina superior-izquierda y la esquina inferior-derecha.\n",
    "        \n",
    "             (box[0],box[1])------------\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    ------------(box[2],box[3])\n",
    "                            \n",
    "    identidades: list\n",
    "        Default: None\n",
    "        Identidad asociada a cada bounding box. Debe tener el mismo número de\n",
    "        elementos que `bboxes` y estar alineados de forma que `identidades[i]`\n",
    "        se corresponde con `bboxes[i]`.\n",
    "        \n",
    "    ax: matplotlib.axes.Axes\n",
    "        Default: None\n",
    "        Axes de matplotlib sobre el que representar la imagen.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
    "        raise Exception(\n",
    "            f\"`imagen` debe ser `np.ndarray, PIL.Image`. Recibido {type(imagen)}.\"\n",
    "        )\n",
    "        \n",
    "    if identidades is not None:\n",
    "        print(f\"f10 {bboxes}\")\n",
    "        print(f\"f11 {identidades}\")\n",
    "        \n",
    "        if len(bboxes) != len(identidades):\n",
    "            raise Exception(\n",
    "                '`identidades` debe tener el mismo número de elementos que `bboxes`.'\n",
    "            )\n",
    "    else:\n",
    "        identidades = [None] * len(bboxes)\n",
    "\n",
    "    # Mostrar la imagen y superponer bounding boxes\n",
    "    # --------------------------------------------------------------------------\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    if isinstance(imagen, PIL.Image.Image):\n",
    "        imagen = np.array(imagen).astype(np.float32) / 255\n",
    "        \n",
    "    ax.imshow(imagen)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    if len(bboxes) > 0:\n",
    "        \n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            if identidades[i] is not None:\n",
    "                rect = plt.Rectangle(\n",
    "                            xy        = (bbox[0], bbox[1]),\n",
    "                            width     = bbox[2] - bbox[0],\n",
    "                            height    = bbox[3] - bbox[1],\n",
    "                            linewidth = 1,\n",
    "                            edgecolor = 'lime',\n",
    "                            facecolor = 'none'\n",
    "                        )\n",
    "                \n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                ax.text(\n",
    "                    x = bbox[0],\n",
    "                    y = bbox[1] -10,\n",
    "                    s = identidades[i],\n",
    "                    fontsize = 10,\n",
    "                    color    = 'lime'\n",
    "                )\n",
    "            else:\n",
    "                rect = plt.Rectangle(\n",
    "                            xy        = (bbox[0], bbox[1]),\n",
    "                            width     = bbox[2] - bbox[0],\n",
    "                            height    = bbox[3] - bbox[1],\n",
    "                            linewidth = 1,\n",
    "                            edgecolor = 'red',\n",
    "                            facecolor = 'none'\n",
    "                        )\n",
    "                \n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def mostrar_bboxes_cv2(imagen: Union[PIL.Image.Image, np.ndarray],\n",
    "                       bboxes: np.ndarray,\n",
    "                       identidades: list=None,\n",
    "                       device: str='window') -> None:\n",
    "    \"\"\"\n",
    "    Mostrar la imagen original con las boundig box de las caras detectadas\n",
    "    empleando OpenCV. Si pasa las identidades, se muestran sobre cada\n",
    "    bounding box. Esta función no puede utilizarse dentro de un Jupyter notebook.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    imagen: PIL.Image, np.ndarray\n",
    "        `PIL Image` o `numpy array` con la representación de la imagen.\n",
    "    \n",
    "    bboxes: np.array\n",
    "        Numpy array con las bounding box de las caras presentes en las imágenes.\n",
    "        Cada bounding box es a su vez una array formada por 4 valores que definen\n",
    "        las coordenadas de la esquina superior-izquierda y la esquina inferior-derecha.\n",
    "        \n",
    "             (box[0],box[1])------------\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    ------------(box[2],box[3])\n",
    "                            \n",
    "    identidades: list\n",
    "        Default: None\n",
    "        Identidad asociada a cada bounding box. Debe tener el mismo número de\n",
    "        elementos que `bboxes` y estar alineados de forma que `identidades[i]`\n",
    "        se corresponde con `bboxes[i]`.\n",
    "        \n",
    "    devide: str\n",
    "        Default: 'window'\n",
    "        Nombre de la ventana emergente que abre cv2.imshow(). Si `None`, se \n",
    "        devuelve la imagen pero no se muestra en ventana.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
    "        raise Exception(\n",
    "            f\"`imagen` debe ser `np.ndarray`, `PIL.Image`. Recibido {type(imagen)}.\"\n",
    "        )\n",
    "        \n",
    "    if identidades is not None:\n",
    "        if len(bboxes) != len(identidades):\n",
    "            raise Exception(\n",
    "                '`identidades` debe tener el mismo número de elementos que `bboxes`.'\n",
    "            )\n",
    "    else:\n",
    "        identidades = [None] * len(bboxes)\n",
    "\n",
    "    # Mostrar la imagen y superponer bounding boxes\n",
    "    # --------------------------------------------------------------------------      \n",
    "    if isinstance(imagen, PIL.Image.Image):\n",
    "        imagen = np.array(imagen).astype(np.float32) / 255\n",
    "    \n",
    "    if len(bboxes) > 0:\n",
    "        \n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            \n",
    "            if identidades[i] is not None:\n",
    "                cv2.rectangle(\n",
    "                    img       = imagen,\n",
    "                    pt1       = (bbox[0], bbox[1]),\n",
    "                    pt2       = (bbox[2], bbox[3]),\n",
    "                    color     = (0, 255, 0),\n",
    "                    thickness = 2\n",
    "                )\n",
    "                \n",
    "                cv2.putText(\n",
    "                    img       = imagen, \n",
    "                    text      = identidades[i], \n",
    "                    org       = (bbox[0], bbox[1]-10), \n",
    "                    fontFace  = cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    fontScale = 1e-3 * imagen.shape[0],\n",
    "                    color     = (0,255,0),\n",
    "                    thickness = 2\n",
    "                )\n",
    "            else:\n",
    "                cv2.rectangle(\n",
    "                    img       = imagen,\n",
    "                    pt1       = (bbox[0], bbox[1]),\n",
    "                    pt2       = (bbox[2], bbox[3]),\n",
    "                    color     = (255, 0, 0),\n",
    "                    thickness = 2\n",
    "                )\n",
    "        \n",
    "    if device is None:\n",
    "        return imagen\n",
    "    else:\n",
    "        cv2.imshow(device, cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB))\n",
    "        if cv2.waitKey(1) == 27: \n",
    "            brcv2.destroyAllWindows()  # esc para cerrar la ventana\n",
    "        \n",
    "        \n",
    "def extraer_caras(imagen: Union[PIL.Image.Image, np.ndarray],\n",
    "                  bboxes: np.ndarray,\n",
    "                  output_img_size: Union[list, tuple, np.ndarray]=[160, 160]) -> None:\n",
    "    \"\"\"\n",
    "    Extraer las zonas de una imagen contenidas en bounding boxes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    imagen: PIL.Image, np.ndarray\n",
    "        PIL Image o numpy array con la representación de la imagen.\n",
    "    \n",
    "    bboxes: np.array\n",
    "        Numpy array con las bounding box de las caras presentes en las imágenes.\n",
    "        Cada bounding box es a su vez una array formada por 4 valores que definen\n",
    "        las coordenadas de la esquina superior-izquierda y la esquina inferior-derecha.\n",
    "        \n",
    "             (box[0],box[1])------------\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    |                  |\n",
    "                    ------------(box[2],box[3])\n",
    "                            \n",
    "    output_img_size: list, tuple, np.ndarray\n",
    "        Default: [160, 160]\n",
    "        Tamaño de las imágenes de salida en pixels.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    np.ndarray, shape=[len(bboxes), output_img_size[0], output_img_size[1], 3]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not isinstance(imagen, (np.ndarray, PIL.Image.Image)):\n",
    "        raise Exception(\n",
    "            f\"`imagen` debe ser np.ndarray, PIL.Image. Recibido {type(imagen)}.\"\n",
    "        )\n",
    "        \n",
    "    # Recorte de cara\n",
    "    # --------------------------------------------------------------------------\n",
    "    if isinstance(imagen, PIL.Image.Image):\n",
    "        imagen = np.array(imagen)\n",
    "        \n",
    "    if len(bboxes) > 0:\n",
    "        caras = []\n",
    "        for bbox in bboxes:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            cara = imagen[y1:y2, x1:x2]\n",
    "            # Redimensionamiento del recorte\n",
    "            cara = Image.fromarray(cara)\n",
    "            cara = cara.resize(tuple(output_img_size))\n",
    "            cara = np.array(cara)\n",
    "            caras.append(cara)\n",
    "            \n",
    "    caras = np.stack(caras, axis=0)\n",
    "\n",
    "    return caras\n",
    "\n",
    "\n",
    "def calcular_embeddings(img_caras: np.ndarray, encoder=None,\n",
    "                        device: str=None) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Caclular el embedding (encoding) de caras utilizando el modelo InceptionResnetV1\n",
    "    de la librería facenet_pytorch. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    img_caras: np.ndarray, shape=[nº caras, ancho, alto, 3]\n",
    "        Imágenes que representan las caras.\n",
    "    \n",
    "    encoder : facenet_pytorch.models.inception_resnet_v1.InceptionResnetV1\n",
    "        Default: None\n",
    "        Modelo ``InceptionResnetV1`` empleado para obtener el embedding numérico\n",
    "        de las caras. Si es ``None`` se inicializa uno nuevo. La inicialización \n",
    "        del modelo puede tardar varios segundos, por lo que, en ciertos escenarios,\n",
    "        es preferible inicializarlo al principio del script y pasarlo como argumeto.\n",
    "        \n",
    "    device: str\n",
    "        Default: None\n",
    "        Device donde se ejecuta el modelo. Si el encoder, se pasa como argumento,\n",
    "        no es necesario.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    np.ndarray, shape=[nº caras, 512]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not isinstance(img_caras, np.ndarray):\n",
    "        raise Exception(\n",
    "            f\"`img_caras` debe ser np.ndarray {type(img_caras)}.\"\n",
    "        )\n",
    "        \n",
    "    if img_caras.ndim != 4:\n",
    "        raise Exception(\n",
    "            f\"`img_caras` debe ser np.ndarray con dimensiones [nº caras, ancho, alto, 3].\"\n",
    "            f\" Recibido {img_caras.ndim}.\"\n",
    "        )\n",
    "        \n",
    "    if encoder is None:\n",
    "        logging.info('Iniciando encoder InceptionResnetV1')\n",
    "        encoder = InceptionResnetV1(\n",
    "                        pretrained = 'vggface2',\n",
    "                        classify   = False,\n",
    "                        device     = device\n",
    "                   ).eval()\n",
    "        \n",
    "    # Calculo de embedings\n",
    "    # --------------------------------------------------------------------------\n",
    "    # El InceptionResnetV1 modelo requiere que las dimensiones de entrada sean\n",
    "    # [nº caras, 3, ancho, alto]\n",
    "    caras = np.moveaxis(img_caras, -1, 1)\n",
    "    caras = caras.astype(np.float32) / 255\n",
    "    caras = torch.tensor(caras)\n",
    "    embeddings = encoder.forward(caras).detach().cpu().numpy()\n",
    "    embeddings = embeddings\n",
    "    #return embeddings\n",
    "    embeddings_flat = embeddings.flatten()\n",
    "    print(f\"f1 522 : {embeddings_flat}\")\n",
    "    return embeddings_flat\n",
    "\n",
    "\n",
    "def identificar_caras(embeddings: np.ndarray,\n",
    "                      dic_referencia: dict,\n",
    "                      threshold_similaridad: float = 0.6) -> list:\n",
    "    \"\"\"\n",
    "    Dado un conjunto de nuevos embeddings y un diccionario con de referencia,\n",
    "    se calcula la similitud entre cada nuevo embedding y los embeddings de\n",
    "    referencias.  Si la similitud supera un determinado threshold se devuelve la\n",
    "    identidad de la persona.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    embeddings: np.ndarray, shape=[nº caras, 512]\n",
    "        Embeddings de las caras que se quieren identificar.\n",
    "        \n",
    "    dic_referencia: dict\n",
    "        Diccionario utilizado como valores de referencia. La clave representa\n",
    "        la identidad de la persona y el valor el embedding de su cara.\n",
    "            \n",
    "    threshold_similaridad: float\n",
    "        Default: 0.6\n",
    "        Similitud mínima que tiene que haber entre embeddings para que se le\n",
    "        asigne la identidad. De lo contrario se le asigna la etiqueta de \"desconocido\".\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    list, len=nº caras\n",
    "    \"\"\"\n",
    "    print(f\"f2 555 : {embeddings}\")\n",
    "    identidades = []\n",
    "        \n",
    "    for i in range(embeddings.shape[0]):\n",
    "        #print(f\"f9 559 {embeddings[i]}\")\n",
    "        # Se calcula la similitud con cada uno de los perfiles de referencia.\n",
    "        similitudes = {}\n",
    "        for key, value in dic_referencia.items():\n",
    "            embedding_flat = embeddings[i].flatten()###########################################\n",
    "            value_flat = value.flatten()#######################################################\n",
    "            #print(\"f9_________________\")#print(f\"f9 564 {value}\")\n",
    "            #print(f\"f9 565 {1 - cosine(embedding_flat, value_flat)}\")\n",
    "            similitudes[key] = 1 - cosine(embedding_flat, value_flat)\n",
    "        \n",
    "        # Se identifica la persona de mayor similitud.\n",
    "        identidad = max(similitudes, key=similitudes.get)\n",
    "        # Si la similitud < threshold_similaridad, se etiqueta como None\n",
    "        if similitudes[identidad] < threshold_similaridad:\n",
    "            identidad = None\n",
    "            \n",
    "        identidades.append(identidad)\n",
    "        \n",
    "    return identidades\n",
    "\n",
    "def crear_diccionario_referencias(folder_path:str,\n",
    "                                  dic_referencia:dict=None,\n",
    "                                  detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
    "                                  min_face_size: int=40,\n",
    "                                  thresholds: list=[0.6, 0.7, 0.7],\n",
    "                                  min_confidence: float=0.9,\n",
    "                                  encoder=None,\n",
    "                                  device: str=None,\n",
    "                                  verbose: bool=False)-> dict:\n",
    "    \"\"\"\n",
    "    Crea o actualiza un diccionario con los embeddings de referencia de personas.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    folder_path: str\n",
    "        Path al directorio con las imágenes de referencia. La estructura esperada\n",
    "        en este directorio es:\n",
    "        \n",
    "            - Una carpeta por cada identidad. El nombre de la carpeta se utiliza\n",
    "              como identificador único.\n",
    "              \n",
    "            - Dentro de cada carpeta puede haber una o más imágenes de la persona.\n",
    "              Si hay más de una se calcula el embedding promedio de todas ellas.\n",
    "              En las imágenes de referencia solo puede aparecer la cara de la \n",
    "              persona en cuestión.\n",
    "    \n",
    "    dic_referencia: dict \n",
    "        Default: None\n",
    "        Diccionario de referencia previamente creado. Se actualiza con las nuevas\n",
    "        identidades. En el caso de identidades ya existentes, se actualizan con\n",
    "        los nuevos embeddings.\n",
    "        \n",
    "    detector : facenet_pytorch.models.mtcnn.MTCNN\n",
    "        Default: None\n",
    "        Modelo ``MTCNN`` empleado para detectar las caras de la imagen. Si es\n",
    "        ``None`` se inicializa uno nuevo. La inicialización del modelo puede\n",
    "        tardar varios segundos, por lo que, en ciertos escenarios, es preferible\n",
    "        inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    min_face_size : int\n",
    "        Default: 40\n",
    "        Tamaño mínimo de que deben tener las caras para ser detectadas por la red \n",
    "        MTCNN.\n",
    "        \n",
    "    thresholds: list\n",
    "        Default: [0.6, 0.7, 0.7]\n",
    "        Límites de detección de cada una de las 3 redes que forman el detector MTCNN.\n",
    "    \n",
    "    min_confidence : float\n",
    "        Default: 0.9\n",
    "        confianza (probabilidad) mínima que debe de tener la cara detectada para\n",
    "        que se incluya en los resultados.\n",
    "        \n",
    "    encoder : facenet_pytorch.models.inception_resnet_v1.InceptionResnetV1\n",
    "        Default: None\n",
    "        Modelo ``InceptionResnetV1`` empleado para obtener el embedding numérico\n",
    "        de las caras. Si es ``None`` se inicializa uno nuevo. La inicialización \n",
    "        del modelo puede tardar varios segundos, por lo que, en ciertos escenarios,\n",
    "        es preferible inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    device: str\n",
    "        Default: None\n",
    "        Device donde se ejecutan los modelos de detección y embedding. Ignorado\n",
    "        si el encoder o el detector han sido inicializados con anterioridad.\n",
    "        \n",
    "    verbose : bool\n",
    "        Default: False\n",
    "        Mostrar información del proceso por pantalla.\n",
    "                \n",
    "    Return\n",
    "    ------\n",
    "    dict\n",
    "        Diccionario con los embeddings de referencia. La clave representa la\n",
    "        identidad de la persona y el valor el embedding de su cara.\n",
    "    \"\"\"\n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not os.path.isdir(folder_path):\n",
    "        raise Exception(\n",
    "            f\"Directorio {folder_path} no existe.\"\n",
    "        )\n",
    "        \n",
    "    if len(os.listdir(folder_path) ) == 0:\n",
    "        raise Exception(\n",
    "            f\"Directorio {folder_path} está vacío.\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "    if detector is None:\n",
    "        logging.info('Iniciando detector MTCC')\n",
    "        detector = MTCNN(\n",
    "                        keep_all      = False,\n",
    "                        post_process  = False,\n",
    "                        min_face_size = min_face_size,\n",
    "                        thresholds    = thresholds,\n",
    "                        device        = device\n",
    "                   )\n",
    "    \n",
    "    if encoder is None:\n",
    "        logging.info('Iniciando encoder InceptionResnetV1')\n",
    "        encoder = InceptionResnetV1(\n",
    "                        pretrained = 'vggface2',\n",
    "                        classify   = False,\n",
    "                        device     = device\n",
    "                   ).eval()\n",
    "        \n",
    "    \n",
    "    new_dic_referencia = {}\n",
    "    folders = glob.glob(folder_path + \"/*\")\n",
    "    \n",
    "    for folder in folders:\n",
    "        \n",
    "        if platform.system() in ['Linux', 'Darwin']:\n",
    "            identidad = folder.split(\"/\")[-1]\n",
    "        else:\n",
    "            identidad = folder.split(\"\\\\\")[-1]\n",
    "                                     \n",
    "        logging.info(f'Obteniendo embeddings de: {identidad}')\n",
    "        embeddings = []\n",
    "        print(f\"f3 697 : {embeddings}\")\n",
    "        # Se lista todas las imagenes .jpg .jpeg .tif .png\n",
    "        path_imagenes = glob.glob(folder + \"/*.jpg\")\n",
    "        path_imagenes.extend(glob.glob(folder + \"/*.jpeg\"))\n",
    "        path_imagenes.extend(glob.glob(folder + \"/*.tif\"))\n",
    "        path_imagenes.extend(glob.glob(folder + \"/*.png\"))\n",
    "        logging.info(f'Total imagenes referencia: {len(path_imagenes)}')\n",
    "        \n",
    "        for path_imagen in path_imagenes:\n",
    "            logging.info(f'Leyendo imagen: {path_imagen}')\n",
    "            imagen = Image.open(path_imagen)\n",
    "            # Si la imagen es RGBA se pasa a RGB\n",
    "            if np.array(imagen).shape[2] == 4:\n",
    "                imagen  = np.array(imagen)[:, :, :3]\n",
    "                imagen  = Image.fromarray(imagen)\n",
    "                \n",
    "            bbox = detectar_caras(\n",
    "                        imagen,\n",
    "                        detector       = detector,\n",
    "                        min_confidence = min_confidence,\n",
    "                        verbose        = False\n",
    "                    )\n",
    "            \n",
    "            if len(bbox) > 1:\n",
    "                logging.warning(\n",
    "                    f'Más de 2 caras detectadas en la imagen: {path_imagen}. '\n",
    "                    f'Se descarta la imagen del diccionario de referencia.'\n",
    "                )\n",
    "                continue\n",
    "                \n",
    "            if len(bbox) == 0:\n",
    "                logging.warning(\n",
    "                    f'No se han detectado caras en la imagen: {path_imagen}.'\n",
    "                )\n",
    "                continue\n",
    "                \n",
    "            cara = extraer_caras(imagen, bbox)\n",
    "            embedding = calcular_embeddings(cara, encoder=encoder)\n",
    "            embeddings.append(embedding)\n",
    "            print(f\"f4 736 : {embeddings}\")\n",
    "        if verbose:\n",
    "            print(f\"Identidad: {identidad} --- Imágenes referencia: {len(embeddings)}\")\n",
    "            \n",
    "        embedding_promedio = np.array(embeddings).mean(axis = 0)\n",
    "        new_dic_referencia[identidad] = embedding_promedio\n",
    "        print(f\"f5 742 : {embedding_promedio}\")\n",
    "    if dic_referencia is not None:\n",
    "        dic_referencia.update(new_dic_referencia)\n",
    "        return dic_referencia\n",
    "    else:\n",
    "        return new_dic_referencia\n",
    "    \n",
    "\n",
    "def pipeline_deteccion_imagen(imagen: Union[PIL.Image.Image, np.ndarray],\n",
    "                              dic_referencia:dict,\n",
    "                              detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
    "                              keep_all: bool=True,\n",
    "                              min_face_size: int=20,\n",
    "                              thresholds: list=[0.6, 0.7, 0.7],\n",
    "                              device: str=None,\n",
    "                              min_confidence: float=0.5,\n",
    "                              fix_bbox: bool=True,\n",
    "                              output_img_size: Union[list, tuple, np.ndarray]=[160, 160],\n",
    "                              encoder=None,\n",
    "                              threshold_similaridad: float=0.5,\n",
    "                              ax=None,\n",
    "                              verbose=False)-> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Detección e identificación de las personas que aparecen en una imagen.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    imagen: PIL.Image, np.ndarray\n",
    "        PIL Image o numpy array con la representación de la imagen.\n",
    "        \n",
    "    dic_referencia: dict \n",
    "        Diccionario con los embeddings de referencia.\n",
    "\n",
    "    detector : facenet_pytorch.models.mtcnn.MTCNN\n",
    "        Default: None\n",
    "        Modelo ``MTCNN`` empleado para detectar las caras de la imagen. Si es\n",
    "        ``None`` se inicializa uno nuevo. La inicialización del modelo puede\n",
    "        tardar varios segundos, por lo que, en ciertos escenarios, es preferible\n",
    "        inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    keep_all: bool\n",
    "        Default: True\n",
    "        Si `True`, se devuelven todas las caras detectadas en la imagen.\n",
    "\n",
    "    min_face_size : int\n",
    "        Default: 20\n",
    "        Tamaño mínimo de que deben tener las caras para ser detectadas por la red \n",
    "        MTCNN.\n",
    "        \n",
    "    thresholds: list\n",
    "        Default: [0.6, 0.7, 0.7]\n",
    "        Límites de detección de cada una de las 3 redes que forman el detector MTCNN.\n",
    "    \n",
    "    device: str\n",
    "        Default: None\n",
    "        Device donde se ejecutan los modelos de detección y embedding. Ignorado\n",
    "        si el encoder o el detector han sido inicializados con anterioridad.\n",
    "\n",
    "    min_confidence : float\n",
    "        Default: 0.5\n",
    "        confianza (probabilidad) mínima que debe de tener la cara detectada para\n",
    "        que se incluya en los resultados.\n",
    "\n",
    "    fix_bbox : bool\n",
    "        Default: True\n",
    "        Acota las dimensiones de las bounding box para que no excedan las\n",
    "        dimensiones de la imagen. Esto evita problemas cuando se intenta\n",
    "        representar las bounding box de caras que están en el margen de la\n",
    "        imagen.\n",
    "        \n",
    "    output_img_size: list, tuple, np.ndarray\n",
    "        Default: [160, 160]\n",
    "        Tamaño de las imágenes de salida en pixels.\n",
    "        \n",
    "    encoder : facenet_pytorch.models.inception_resnet_v1.InceptionResnetV1\n",
    "        Default: None\n",
    "        Modelo ``InceptionResnetV1`` empleado para obtener el embedding numérico\n",
    "        de las caras. Si es ``None`` se inicializa uno nuevo. La inicialización \n",
    "        del modelo puede tardar varios segundos, por lo que, en ciertos escenarios,\n",
    "        es preferible inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    threshold_similaridad: float\n",
    "        Default: 0.5\n",
    "        Similitud mínima que tiene que haber entre embeddings para que se le\n",
    "        asigne la identidad. De lo contrario se le asigna la etiqueta de \"desconocido\".\n",
    "        \n",
    "    ax: matplotlib.axes.Axes\n",
    "        Default: None\n",
    "        Axes de matplotlib sobre el que representar la imagen.\n",
    "\n",
    "    verbose : bool\n",
    "        Default: False\n",
    "        Mostrar información del proceso por pantalla.\n",
    "\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    bboxes = detectar_caras(\n",
    "                imagen         = imagen,\n",
    "                detector       = detector,\n",
    "                keep_all       = keep_all,\n",
    "                min_face_size  = min_face_size,\n",
    "                thresholds     = thresholds,\n",
    "                device         = device,\n",
    "                min_confidence = min_confidence,\n",
    "                fix_bbox       = fix_bbox\n",
    "              )\n",
    "    \n",
    "    if len(bboxes) == 0:\n",
    "        \n",
    "        logging.info('No se han detectado caras en la imagen.')\n",
    "        mostrar_bboxes(\n",
    "            imagen      = imagen,\n",
    "            bboxes      = bboxes,\n",
    "            ax          = ax\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        caras = extraer_caras(\n",
    "                    imagen = imagen,\n",
    "                    bboxes = bboxes\n",
    "                )\n",
    "\n",
    "        embeddings = calcular_embeddings(\n",
    "                        img_caras = caras,\n",
    "                        encoder   = encoder\n",
    "                     )\n",
    "        print(f\"f6 878 : {embeddings}\")\n",
    "        identidades = identificar_caras(\n",
    "                         embeddings     = embeddings,\n",
    "                         dic_referencia = dic_referencias,\n",
    "                         threshold_similaridad = threshold_similaridad\n",
    "                       )\n",
    "\n",
    "        mostrar_bboxes(\n",
    "            imagen      = imagen,\n",
    "            bboxes      = bboxes,\n",
    "            identidades = identidades,\n",
    "            ax          = ax\n",
    "        )\n",
    "    \n",
    "    \n",
    "\n",
    "def pipeline_deteccion_video(path_input_video: str,\n",
    "                             dic_referencia: dict,\n",
    "                             path_output_video: str=os.getcwd(),\n",
    "                             detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
    "                             keep_all: bool=True,\n",
    "                             min_face_size: int=40,\n",
    "                             thresholds: list=[0.6, 0.7, 0.7],\n",
    "                             device: str=None,\n",
    "                             min_confidence: float=0.5,\n",
    "                             fix_bbox: bool=True,\n",
    "                             output_img_size: Union[list, tuple, np.ndarray]=[160, 160],\n",
    "                             encoder=None,\n",
    "                             threshold_similaridad: float=0.5,\n",
    "                             ax=None,\n",
    "                             verbose=False)-> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Detección e identificación de las personas que aparecen en un vídeo. El\n",
    "    resultado se escribe en una ruta especificada por el usuario.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    path_input_video: str\n",
    "        Path completo al vídeo que se quiere procesar\n",
    "        \n",
    "    dic_referencia: dict \n",
    "        Diccionario con los embeddings de referencia.\n",
    "\n",
    "    detector : facenet_pytorch.models.mtcnn.MTCNN\n",
    "        Default: None\n",
    "        Modelo ``MTCNN`` empleado para detectar las caras de la imagen. Si es\n",
    "        ``None`` se inicializa uno nuevo. La inicialización del modelo puede\n",
    "        tardar varios segundos, por lo que, en ciertos escenarios, es preferible\n",
    "        inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    keep_all: bool\n",
    "        Default: True\n",
    "        Si `True`, se devuelven todas las caras detectadas en la imagen.\n",
    "\n",
    "    min_face_size : int\n",
    "        Default: 20\n",
    "        Tamaño mínimo de que deben tener las caras para ser detectadas por la red \n",
    "        MTCNN.\n",
    "        \n",
    "    thresholds: list\n",
    "        Default: [0.6, 0.7, 0.7]\n",
    "        Límites de detección de cada una de las 3 redes que forman el detector MTCNN.\n",
    "    \n",
    "    device: str\n",
    "        Default: None\n",
    "        Device donde se ejecutan los modelos de detección y embedding. Ignorado\n",
    "        si el encoder o el detector han sido inicializados con anterioridad.\n",
    "\n",
    "    min_confidence : float\n",
    "        Default: 0.5\n",
    "        confianza (probabilidad) mínima que debe de tener la cara detectada para\n",
    "        que se incluya en los resultados.\n",
    "\n",
    "    fix_bbox : bool\n",
    "        Default: True\n",
    "        Acota las dimensiones de las bounding box para que no excedan las\n",
    "        dimensiones de la imagen. Esto evita problemas cuando se intenta\n",
    "        representar las bounding box de caras que están en el margen de la\n",
    "        imagen.\n",
    "        \n",
    "    output_img_size: list, tuple, np.ndarray\n",
    "        Default: [160, 160]\n",
    "        Tamaño de las imágenes de salida en pixels.\n",
    "        \n",
    "    encoder : facenet_pytorch.models.inception_resnet_v1.InceptionResnetV1\n",
    "        Default: None\n",
    "        Modelo ``InceptionResnetV1`` empleado para obtener el embedding numérico\n",
    "        de las caras. Si es ``None`` se inicializa uno nuevo. La inicialización \n",
    "        del modelo puede tardar varios segundos, por lo que, en ciertos escenarios,\n",
    "        es preferible inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    threshold_similaridad: float\n",
    "        Default: 0.6\n",
    "        Similitud mínima que tiene que haber entre embeddings para que se le\n",
    "        asigne la identidad. De lo contrario se le asigna la etiqueta de \"desconocido\".\n",
    "        \n",
    "    ax: matplotlib.axes.Axes\n",
    "        Default: None\n",
    "        Axes de matplotlib sobre el que representar la imagen.\n",
    "\n",
    "    verbose : bool\n",
    "        Default: False\n",
    "        Mostrar información del proceso por pantalla.\n",
    "\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Comprobaciones iniciales\n",
    "    # --------------------------------------------------------------------------\n",
    "    if not os.path.isfile(path_input_video):\n",
    "        raise Exception(\n",
    "            f\"El archivo {path_input_video} no existe.\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "    capture = cv2.VideoCapture(path_input_video)\n",
    "    input_frames = []\n",
    "    output_frames = []\n",
    "\n",
    "    frame_exist = True\n",
    "    while(frame_exist):\n",
    "        frame_exist, frame = capture.read()\n",
    "\n",
    "        if not frame_exist:\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_frames.append(frame)\n",
    "    capture.release()\n",
    "\n",
    "\n",
    "    for frame in tqdm(input_frames):\n",
    "\n",
    "        bboxes = detectar_caras(\n",
    "                        imagen         = frame,\n",
    "                        detector       = detector,\n",
    "                        keep_all       = keep_all,\n",
    "                        min_face_size  = min_face_size,\n",
    "                        thresholds     = thresholds,\n",
    "                        device         = device,\n",
    "                        min_confidence = min_confidence,\n",
    "                        fix_bbox       = fix_bbox\n",
    "                      )\n",
    "\n",
    "        if len(bboxes) == 0:\n",
    "\n",
    "            logging.info('No se han detectado caras en la imagen.')\n",
    "\n",
    "            frame_procesado = mostrar_bboxes_cv2(\n",
    "                                imagen      = frame,\n",
    "                                bboxes      = bboxes,\n",
    "                                device      = None\n",
    "                             )\n",
    "            output_frames.append(frame_procesado)\n",
    "\n",
    "        else:\n",
    "\n",
    "            caras = extraer_caras(\n",
    "                        imagen = frame,\n",
    "                        bboxes = bboxes\n",
    "                    )\n",
    "\n",
    "            embeddings = calcular_embeddings(\n",
    "                            img_caras = caras,\n",
    "                            encoder   = encoder\n",
    "                         )\n",
    "            print(f\"f7 1051 : {embeddings}\")\n",
    "            identidades = identificar_caras(\n",
    "                             embeddings     = embeddings,\n",
    "                             dic_referencia = dic_referencias,\n",
    "                             threshold_similaridad = threshold_similaridad\n",
    "                          )\n",
    "\n",
    "            frame_procesado = mostrar_bboxes_cv2(\n",
    "                                imagen      = frame,\n",
    "                                bboxes      = bboxes,\n",
    "                                identidades = identidades,\n",
    "                                device = None\n",
    "                             )\n",
    "            output_frames.append(frame_procesado)\n",
    "            \n",
    "    if len(output_frames) > 0:\n",
    "        frame_size = (output_frames[0].shape[1], output_frames[0].shape[0])\n",
    "        #out = cv2.VideoWriter('test.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, frame_size)\n",
    "        #out = cv2.VideoWriter(path_output_video, 0x7634706d, 25, frame_size)\n",
    "        out = cv2.VideoWriter(path_output_video, cv2.VideoWriter_fourcc(*'MP4V'), 25, frame_size)\n",
    "\n",
    "        for frame in output_frames:\n",
    "            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        out.release()\n",
    "            \n",
    "    return\n",
    "\n",
    "\n",
    "def pipeline_deteccion_webcam(dic_referencia: dict,\n",
    "                             output_device: str = 'window',\n",
    "                             path_output_video: str=os.getcwd(),\n",
    "                             detector: facenet_pytorch.models.mtcnn.MTCNN=None,\n",
    "                             keep_all: bool=True,\n",
    "                             min_face_size: int=40,\n",
    "                             thresholds: list=[0.6, 0.7, 0.7],\n",
    "                             device: str=None,\n",
    "                             min_confidence: float=0.5,\n",
    "                             fix_bbox: bool=True,\n",
    "                             output_img_size: Union[list, tuple, np.ndarray]=[160, 160],\n",
    "                             encoder=None,\n",
    "                             threshold_similaridad: float=0.5,\n",
    "                             ax=None,\n",
    "                             verbose=False)-> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Detección e identificación de las personas que aparecen en el vídeo de webcam.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    output_device: str\n",
    "        Nombre de la ventana emergente que abre cv2.imshow()\n",
    "        \n",
    "    dic_referencia: dict \n",
    "        Diccionario con los embeddings de referencia.\n",
    "\n",
    "    detector : facenet_pytorch.models.mtcnn.MTCNN\n",
    "        Default: None\n",
    "        Modelo ``MTCNN`` empleado para detectar las caras de la imagen. Si es\n",
    "        ``None`` se inicializa uno nuevo. La inicialización del modelo puede\n",
    "        tardar varios segundos, por lo que, en ciertos escenarios, es preferible\n",
    "        inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    keep_all: bool\n",
    "        Default: True\n",
    "        Si `True`, se devuelven todas las caras detectadas en la imagen.\n",
    "\n",
    "    min_face_size : int\n",
    "        Default: 20\n",
    "        Tamaño mínimo de que deben tener las caras para ser detectadas por la red \n",
    "        MTCNN.\n",
    "        \n",
    "    thresholds: list\n",
    "        Default: [0.6, 0.7, 0.7]\n",
    "        Límites de detección de cada una de las 3 redes que forman el detector MTCNN.\n",
    "    \n",
    "    device: str\n",
    "        Default: None\n",
    "        Device donde se ejecutan los modelos de detección y embedding. Ignorado\n",
    "        si el encoder o el detector han sido inicializados con anterioridad.\n",
    "\n",
    "    min_confidence : float\n",
    "        Default: 0.5\n",
    "        confianza (probabilidad) mínima que debe de tener la cara detectada para\n",
    "        que se incluya en los resultados.\n",
    "\n",
    "    fix_bbox : bool\n",
    "        Default: True\n",
    "        Acota las dimensiones de las bounding box para que no excedan las\n",
    "        dimensiones de la imagen. Esto evita problemas cuando se intenta\n",
    "        representar las bounding box de caras que están en el margen de la\n",
    "        imagen.\n",
    "        \n",
    "    output_img_size: list, tuple, np.ndarray\n",
    "        Default: [160, 160]\n",
    "        Tamaño de las imágenes de salida en pixels.\n",
    "        \n",
    "    encoder : facenet_pytorch.models.inception_resnet_v1.InceptionResnetV1\n",
    "        Default: None\n",
    "        Modelo ``InceptionResnetV1`` empleado para obtener el embedding numérico\n",
    "        de las caras. Si es ``None`` se inicializa uno nuevo. La inicialización \n",
    "        del modelo puede tardar varios segundos, por lo que, en ciertos escenarios,\n",
    "        es preferible inicializarlo al principio del script y pasarlo como argumento.\n",
    "        \n",
    "    threshold_similaridad: float\n",
    "        Default: 0.6\n",
    "        Similitud mínima que tiene que haber entre embeddings para que se le\n",
    "        asigne la identidad. De lo contrario se le asigna la etiqueta de \"desconocido\".\n",
    "        \n",
    "    ax: matplotlib.axes.Axes\n",
    "        Default: None\n",
    "        Axes de matplotlib sobre el que representar la imagen.\n",
    "\n",
    "    verbose : bool\n",
    "        Default: False\n",
    "        Mostrar información del proceso por pantalla.\n",
    "\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    frame_exist = True\n",
    "\n",
    "    while(frame_exist):\n",
    "        frame_exist, frame = capture.read()\n",
    "\n",
    "        if not frame_exist:\n",
    "            capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        bboxes = detectar_caras(\n",
    "                        imagen         = frame,\n",
    "                        detector       = detector,\n",
    "                        keep_all       = keep_all,\n",
    "                        min_face_size  = min_face_size,\n",
    "                        thresholds     = thresholds,\n",
    "                        device         = device,\n",
    "                        min_confidence = min_confidence,\n",
    "                        fix_bbox       = fix_bbox\n",
    "                      )\n",
    "\n",
    "        if len(bboxes) == 0:\n",
    "\n",
    "            logging.info('No se han detectado caras en la imagen.')\n",
    "            cv2.imshow(output_device, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                             \n",
    "        else:\n",
    "\n",
    "            caras = extraer_caras(\n",
    "                        imagen = frame,\n",
    "                        bboxes = bboxes\n",
    "                    )\n",
    "\n",
    "            embeddings = calcular_embeddings(\n",
    "                            img_caras = caras,\n",
    "                            encoder   = encoder\n",
    "                         )\n",
    "            print(f\"f8 1051 : {embeddings}\")\n",
    "            identidades = identificar_caras(\n",
    "                             embeddings     = embeddings,\n",
    "                             dic_referencia = dic_referencias,\n",
    "                             threshold_similaridad = threshold_similaridad\n",
    "                          )\n",
    "\n",
    "            frame_procesado = mostrar_bboxes_cv2(\n",
    "                                imagen      = frame,\n",
    "                                bboxes      = bboxes,\n",
    "                                identidades = identidades,\n",
    "                                device = output_device\n",
    "                             )\n",
    "            \n",
    "        if cv2.waitKey(1) == 27: \n",
    "            break  # esc para cerrar la ventana\n",
    "\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb7f514",
   "metadata": {},
   "source": [
    "# Creación del diccionario de referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dd878d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga de las imágenes de referencia\n",
    "# ==============================================================================\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "url = ('https://github.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/'\n",
    "       'raw/master/images/imagenes_referencia_reconocimiento_facial.zip')\n",
    "\n",
    "extract_dir = './images/imagenes_referencia_reconocimiento_facial'\n",
    "\n",
    "zip_path, _ = urllib.request.urlretrieve(url)\n",
    "with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "    f.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34a06282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "Identidad: AlexDunphy --- Imágenes referencia: 3\n",
      "Identidad: CameronTucker --- Imágenes referencia: 2\n",
      "Identidad: ClaireDunphy --- Imágenes referencia: 3\n",
      "Identidad: GloriaPritchett --- Imágenes referencia: 3\n",
      "Identidad: HaleyDunphy --- Imágenes referencia: 4\n",
      "Identidad: JayPritchett --- Imágenes referencia: 2\n",
      "Identidad: JoePritchett --- Imágenes referencia: 5\n",
      "Identidad: LilyTucker-Pritchett --- Imágenes referencia: 4\n",
      "Identidad: LukeDunphy --- Imágenes referencia: 4\n",
      "Identidad: MannyDelgado --- Imágenes referencia: 6\n",
      "Identidad: MitchellPritchett --- Imágenes referencia: 3\n",
      "Identidad: PhilDunphy --- Imágenes referencia: 4\n"
     ]
    }
   ],
   "source": [
    "# Detectar si se dispone de GPU cuda\n",
    "# ==============================================================================\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(F'Running on device: {device}')\n",
    "\n",
    "# Crear diccionario de referencia para cada persona\n",
    "# ==============================================================================\n",
    "dic_referencias = crear_diccionario_referencias(\n",
    "                    folder_path    = './images/imagenes_referencia_reconocimiento_facial',\n",
    "                    min_face_size  = 40,\n",
    "                    min_confidence = 0.9,\n",
    "                    device         = device,\n",
    "                    verbose        = True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a402e",
   "metadata": {},
   "source": [
    "# Reconocimiento en imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "341229c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "f1 522 : [ 0.06239086  0.01529616 -0.02054734 ... -0.05574317  0.03377815\n",
      "  0.02158951]\n",
      "f6 878 : [ 0.06239086  0.01529616 -0.02054734 ... -0.05574317  0.03377815\n",
      "  0.02158951]\n",
      "f2 555 : [ 0.06239086  0.01529616 -0.02054734 ... -0.05574317  0.03377815\n",
      "  0.02158951]\n",
      "f10 [[ 293   64  402  194]\n",
      " [ 505   89  605  224]\n",
      " [ 108   95  210  227]\n",
      " [ 427  207  529  333]\n",
      " [  47  235  145  361]\n",
      " [1069  134 1165  262]\n",
      " [ 682  126  778  248]\n",
      " [ 659  291  750  402]\n",
      " [ 886  128  968  250]\n",
      " [ 239  245  326  355]\n",
      " [ 931  496 1012  613]\n",
      " [ 816  663  889  751]]\n",
      "f11 [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "`identidades` debe tener el mismo número de elementos que `bboxes`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3140\\2409960349.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mimagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'modernfamily.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m pipeline_deteccion_imagen(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mimagen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimagen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdic_referencia\u001b[0m        \u001b[1;33m=\u001b[0m \u001b[0mdic_referencias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3140\\3609812309.py\u001b[0m in \u001b[0;36mpipeline_deteccion_imagen\u001b[1;34m(imagen, dic_referencia, detector, keep_all, min_face_size, thresholds, device, min_confidence, fix_bbox, output_img_size, encoder, threshold_similaridad, ax, verbose)\u001b[0m\n\u001b[0;32m    888\u001b[0m                        )\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         mostrar_bboxes(\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[0mimagen\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mimagen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m             \u001b[0mbboxes\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3140\\3609812309.py\u001b[0m in \u001b[0;36mmostrar_bboxes\u001b[1;34m(imagen, bboxes, identidades, ax)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentidades\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             raise Exception(\n\u001b[0m\u001b[0;32m    247\u001b[0m                 \u001b[1;34m'`identidades` debe tener el mismo número de elementos que `bboxes`.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             )\n",
      "\u001b[1;31mException\u001b[0m: `identidades` debe tener el mismo número de elementos que `bboxes`."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reconocimiento en imágenes\n",
    "# ==============================================================================\n",
    "# Detectar si se dispone de GPU cuda\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(F'Running on device: {device}')\n",
    "\n",
    "# Identificar las personas en la imagen\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "imagen = Image.open('modernfamily.jpg')\n",
    "\n",
    "pipeline_deteccion_imagen(\n",
    "    imagen = imagen,\n",
    "    dic_referencia        = dic_referencias,\n",
    "    min_face_size         = 20,\n",
    "    thresholds            = [0.6, 0.7, 0.7],\n",
    "    min_confidence        = 0.5,\n",
    "    threshold_similaridad = 0.6,\n",
    "    device                = device,\n",
    "    ax                    = ax,\n",
    "    verbose               = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63588185",
   "metadata": {},
   "source": [
    "# Reconocimiento en vídeo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d1554",
   "metadata": {},
   "source": [
    "A diferencia de pipeline_deteccion_imagen(), la función pipeline_deteccion_video() no recibe como entrada directamente el vídeo, sino la ruta de su localización. Una vez procesado el vídeo, se escribe de nuevo la salida en disco.\n",
    "\n",
    "El procesamiento de vídeo requiere procesar cada uno de sus frames, por lo que es computacionalmente muy costoso. Se recomienda utilizar GPUs.\n",
    "\n",
    "El vídeo utilizado para este ejemplo puede descargarse del siguiente link.\n",
    "https://github.com/JoaquinAmatRodrigo/Estadistica-machine-learning-python/raw/master/images/video_modern_family.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconocimiento en vídeo\n",
    "# ==============================================================================\n",
    "pipeline_deteccion_video(\n",
    "    path_input_video      = 'videos/video_modern_family.mp4',\n",
    "    path_output_video     = 'videos/video_processed.mp4',\n",
    "    dic_referencia        = dic_referencias,\n",
    "    threshold_similaridad = 0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4342121",
   "metadata": {},
   "source": [
    "Reconocimiento en webcam\n",
    "\n",
    "\n",
    "pipeline_deteccion_webcam() requiere abrir una ventana emergente de visualización, por lo que no puede utilizarse en google colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a99755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_deteccion_webcam(\n",
    "#     dic_referencia        = dic_referencias,\n",
    "#     threshold_similaridad = 0.4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dfca4",
   "metadata": {},
   "source": [
    "Bibliografia y creditos a joaquin Rodrigo.\n",
    "\n",
    "tomado de : https://cienciadedatos.net/documentos/py34-reconocimiento-facial-deeplearning-python\n",
    "\n",
    "Amat Rodrigo, J. (2021). cienciadedatos.net (1.0.0). Zenodo. https://doi.org/10.5281/zenodo.10006330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e0ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
